{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKKLoXdRmMJL",
        "outputId": "e1c5426b-93ee-4b51-d4ac-9042ade42ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4rkzM3M1oMjc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from typing import Tuple, List\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import os\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, source: str, target: str, flip: bool, bs: int, im_size: Tuple[int], make_valid: bool = True):\n",
        "        if not flip:\n",
        "            self.source=source\n",
        "            self.target=target\n",
        "        else:\n",
        "            self.source=target\n",
        "            self.target=source\n",
        "        self.flip=flip\n",
        "        self.bs=bs\n",
        "        self.im_size = im_size\n",
        "        self.dlSourceTrain=None\n",
        "        self.dlTargetTrain=None\n",
        "        self.dlSourceTest=None\n",
        "        self.dlTargetTest=None\n",
        "        # seems like it is not necessary to create validation set\n",
        "        # self.dlSourceValid=None\n",
        "        # self.dlTargetValid=None\n",
        "        # self.make_valid = make_valid\n",
        "\n",
        "    def get_loaders(self, data_dir_name: str) -> List[DataLoader]:\n",
        "        data_dir_path = os.path.join('drive', 'MyDrive', 'data', data_dir_name)\n",
        "        for dir in os.listdir(data_dir_path):\n",
        "            # print(dir)\n",
        "            loader = create_dataloader(os.path.join(data_dir_path, dir), self.im_size, self.bs)\n",
        "            # set the data class with corresponding loader\n",
        "            if dir == f'{self.source}_test':\n",
        "                self.dlSourceTest=loader\n",
        "            elif dir == f'{self.target}_test':\n",
        "                self.dlTargetTest=loader\n",
        "            elif dir == f'{self.source}_train':\n",
        "                self.dlSourceTrain=loader\n",
        "            elif dir == f'{self.target}_train':\n",
        "                self.dlTargetTrain=loader\n",
        "\n",
        "\n",
        "def create_dataloader(root: str, im_size: Tuple, bs: int) -> DataLoader:\n",
        "    dataset = datasets.ImageFolder(\n",
        "        root=root,\n",
        "        transform=transforms.Compose([\n",
        "            # get the images in desired dimensions\n",
        "            transforms.Resize(im_size),\n",
        "            transforms.CenterCrop(im_size),\n",
        "            # turn images to tensors\n",
        "            transforms.ToTensor(),\n",
        "            # apply normalization to mean of 0 and standard deviation of 1\n",
        "            transforms.Normalize((0, 0, 0), (1, 1, 1))\n",
        "        ])\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "def save_images(img_tensors: torch.Tensor, file_name: str, folder: str='output'):\n",
        "    fig, axs = plt.subplots(img_tensors.shape[0], figsize=(5, img_tensors.shape[0]))\n",
        "    img_idx = 0\n",
        "\n",
        "    if img_tensors.shape[0] == 1:\n",
        "        axs.imshow(img_tensors[img_idx].permute(1, 2, 0).detach().cpu())\n",
        "        axs.set_xticks([])\n",
        "        axs.set_yticks([])\n",
        "    else:\n",
        "        for ax in axs:\n",
        "            # plt.axis('off')\n",
        "            ax.imshow(img_tensors[img_idx].permute(1, 2, 0).detach().cpu())\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            img_idx += 1\n",
        "    \n",
        "    # save the figure results\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    plt.savefig(os.path.join(folder, file_name))\n",
        "\n",
        "def show_batch(data: Data, n: int, folder: str='output') -> matplotlib.image.AxesImage:\n",
        "    if n <= 0:\n",
        "        raise ValueError(f\"Expected the number of samples to be >= 1 but got {n} instead\")\n",
        "    elif n > data.bs:\n",
        "        print(f\"Will only show max of {data.bs} samples\")\n",
        "    \n",
        "    sqrt_num_res = n ** (1/2)\n",
        "    if sqrt_num_res - int(sqrt_num_res) != 0:\n",
        "        raise Exception('Num res should be a perfect square (eg. 1, 4, 9, 16, 25, 36...)')\n",
        "    sqrt_num_res = int(sqrt_num_res)\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    for i, loader in enumerate([data.dlSourceTrain, data.dlTargetTrain]):\n",
        "        for x, _ in loader:\n",
        "            fig, axs = plt.subplots(sqrt_num_res, sqrt_num_res, figsize=(5, 5))\n",
        "            img_idx = 0\n",
        "\n",
        "            for row in axs:\n",
        "                for ax in row:\n",
        "                    # plt.axis('off')\n",
        "                    ax.imshow(x[img_idx].squeeze(0).permute(1, 2, 0).detach())\n",
        "                    ax.set_xticks([])\n",
        "                    ax.set_yticks([])\n",
        "                    img_idx += 1\n",
        "            \n",
        "            # save the figure results\n",
        "            if i == 0:\n",
        "                file = data.source+'.png'\n",
        "            else:\n",
        "                file = data.target+'.png'\n",
        "            plt.savefig(os.path.join(folder, file))\n",
        "            break\n",
        "    \n",
        "def set_requires_grad(net, requires_grad):\n",
        "    \"\"\"\n",
        "    Freezes or unfreezes a network (for the weight to change or not during training)\n",
        "    Note that this is pass by reference so returns None\n",
        "    \"\"\"\n",
        "    for param in net.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "\n",
        "def report_losses(losses, loss_names, epoch, time, scale=1):\n",
        "    \"\"\"\n",
        "    Formats losses in a list\n",
        "    \"\"\"\n",
        "    print(f'\\nEpoch: {epoch}', end=\" | \")\n",
        "    for loss, name in zip(losses, loss_names):\n",
        "        print(f\"{name}: {round(loss/scale, 2)}\", end=\" | \")\n",
        "    print(f\"Time: {round(time, 2)}s\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HB-obME_of9T"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# get an instance of a generator to use for numpy random calls\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    \"\"\"\n",
        "    Normalization layer taken from https://github.com/taesungp/contrastive-unpaired-translation/blob/57430e99df041515c57a7ffd18bb7cbc3c1af0a9/models/networks.py#L449\n",
        "\n",
        "    The CUT GAN paper states \"We normalize vectors onto a unit sphere to prevent the space from collapsing or expanding\"\n",
        "    This layer is used to normalize vectors onto a unit sphere by using l2 norm\n",
        "    \"\"\"\n",
        "    def __init__(self, power=2):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute the l2 vector norm\n",
        "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
        "        # scale the input x by the norm\n",
        "        out = x.div(norm + 1e-7)\n",
        "        return out\n",
        "\n",
        "def _single_conv(ch_in, ch_out, ks, stride=1, act=True, gammaZero=False, norm='instance', transpose=False, leaky=False):\n",
        "    \"\"\"\n",
        "    Layer to perform a single convolution, normalization (batch or instance) and activation function (relu and leaky relu)\n",
        "    \"\"\"\n",
        "    # do not reduce size due to ks mismatch\n",
        "    padding = ks//2\n",
        "    if not transpose:\n",
        "        layers = [nn.Conv2d(ch_in, ch_out, ks, stride=stride, padding=padding)]\n",
        "    else:\n",
        "        layers = [nn.ConvTranspose2d(ch_in, ch_out, ks, stride=stride, padding=padding)]\n",
        "\n",
        "    # add norm layer to prevent activations from getting too high\n",
        "    if norm=='instance':\n",
        "        norm_layer = nn.InstanceNorm2d(ch_out, affine=False, track_running_stats=False)\n",
        "    elif norm=='batch':\n",
        "        norm_layer = nn.BatchNorm2d(ch_out, affine=True, track_running_stats=True)\n",
        "    else:\n",
        "        raise Exception(f'Norm should be either \"instance\" or \"batch\" but {norm} was passed')\n",
        "\n",
        "    if gammaZero and norm_layer=='batch':\n",
        "        # init batch norm gamma param to zero to speed up training \n",
        "        nn.init.zeros_(norm_layer.weight.data)\n",
        "\n",
        "    layers.append(norm_layer)\n",
        "    # check if this layer should have an activation - yes unless the final layer\n",
        "    if act and not leaky:\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "    elif act and leaky:\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "    \n",
        "    layers = nn.Sequential(*layers)\n",
        "    return layers\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual blocks to be used by both the generator and discriminator\n",
        "    \"\"\"\n",
        "    def __init__(self, ch_in, ch_out, stride=1, leaky=False):\n",
        "        super().__init__()\n",
        "        self.conv = self._resblock_conv(ch_in, ch_out, leaky, stride=stride)\n",
        "        self.pool = self._return if stride==1 else nn.AvgPool2d(stride, ceil_mode=True)\n",
        "        self.id_conv = self._return if ch_in == ch_out else _single_conv(ch_in, ch_out, 1, stride=1, act=False)\n",
        "        if leaky:\n",
        "            self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        else:\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def _return(self, x):\n",
        "        return x\n",
        "\n",
        "    def _resblock_conv(self, ch_in, ch_out, leaky, stride=1, ks=3):\n",
        "        # create the convolutional path of the resnet block following the bottleneck apporach\n",
        "        conv_block = nn.Sequential(\n",
        "            _single_conv(ch_in, ch_out//4, 1, leaky=leaky),\n",
        "            _single_conv(ch_out//4, ch_out//4, ks, stride=stride, leaky=leaky), \n",
        "            _single_conv(ch_out//4, ch_out, 1, act=False, gammaZero=True)\n",
        "        )\n",
        "        return conv_block\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # apply a skip connection for resnet in the forward call\n",
        "        return self.relu(self.conv(x) + self.id_conv(self.pool(x)))\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Create a generator model (both encoder and decoder) which uses a resnet architecture as the encoder and transpoed 2d convolutions for decoder\n",
        "\n",
        "    Note that sometimes only certain layers will be taken from the encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch, nce_layers, base_ch=64, n_blocks=6, n_downsamples=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_ch: Number of input channels in input tensor image\n",
        "            out_ch: Number of output channels for output tensor image\n",
        "            nce_layers: A list of layers that the patchNCE loss function will be using from the encoder\n",
        "            base_ch: Base number of channels throughout network\n",
        "            n_blocks: Number of residual blocks to use\n",
        "            n_downsamples: Number of downsamples to apply in the network stem\n",
        "        \"\"\"\n",
        "        # network attributes\n",
        "        super().__init__()\n",
        "        self.n_downsamples = n_downsamples\n",
        "        self.n_blocks = n_blocks\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "        self.base_ch = base_ch\n",
        "\n",
        "        # create resnet encoder stem to downsample data n_downsample times and reach initial filter param\n",
        "        # below should be: [in_ch, base_ch//2, base_ch//2, base_ch, base_ch*2, base_ch*4]\n",
        "        stem_sizes = self._create_stem_sizes()\n",
        "        # print(stem_sizes)\n",
        "        self.stem = self._create_stem(stem_sizes)\n",
        "\n",
        "        # Add residual layers for resnet encoder\n",
        "        self.res_layers = self._create_res_layers()\n",
        "\n",
        "        # create the encoder and decoder for the resnet\n",
        "        self.encoder = nn.Sequential(*self.stem+self.res_layers)\n",
        "        # print(self.encoder)\n",
        "        # create upsampling layers for the decoder\n",
        "        anti_stem_sizes = stem_sizes[::-1]\n",
        "        self.decoder = self._create_decoder(anti_stem_sizes)\n",
        "\n",
        "        # determine the layer channels for the Hl (feature extractor) network\n",
        "        self.nce_layers = nce_layers\n",
        "        self._determine_layer_channels(nce_layers)\n",
        "    \n",
        "    def _create_stem_sizes(self):\n",
        "        sizes = [self.in_ch, self.base_ch//2, self.base_ch//2, self.base_ch]\n",
        "        # add the extra sizes as done in the following repo: https://github.com/taesungp/contrastive-unpaired-translation/blob/57430e99df041515c57a7ffd18bb7cbc3c1af0a9/models/networks.py#L914\n",
        "        for i in range(self.n_downsamples):\n",
        "            mult = 2 ** i\n",
        "            sizes.append(self.base_ch*mult*2)\n",
        "        return sizes\n",
        "    \n",
        "    \n",
        "    def _create_stem(self, sizes):\n",
        "        # apply n_downsample stride 2 convolutions \n",
        "        stem = [\n",
        "            _single_conv(sizes[i], sizes[i+1], 3, stride = 2 if i < self.n_downsamples else 1) \n",
        "            for i in range(len(sizes)-1)\n",
        "        ]\n",
        "        return stem\n",
        "    \n",
        "    def _create_res_layers(self):\n",
        "        # create multiplication factor\n",
        "        mult = 2**self.n_downsamples\n",
        "        layers = [ResBlock(self.base_ch*mult, self.base_ch*mult) for i in range(self.n_blocks)]\n",
        "        return layers\n",
        "    \n",
        "    def _create_decoder(self, sizes):\n",
        "        # print(sizes)\n",
        "        lt_downsamples = lambda i: i < self.n_downsamples\n",
        "        # create the decoder with transposed convolutions and a final Tanh layer\n",
        "        decoder = [\n",
        "            *[_single_conv(sizes[i], sizes[i+1], 3 if lt_downsamples(i) else 4, stride = 2 if lt_downsamples(i) else 1, transpose = True if lt_downsamples(i) else False)\n",
        "            for i in range(len(sizes)-1)],\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "        return nn.Sequential(*decoder)\n",
        "    \n",
        "    def _determine_layer_channels(self, layers):\n",
        "        \"\"\"\n",
        "        Determines the channels for the output layers that generator encoder will produce\n",
        "        \"\"\"\n",
        "        # create a list of channels that will be used by the feature extractor\n",
        "        self.feature_extractor_channels = []\n",
        "        for layer_id, layer in enumerate(self.encoder):\n",
        "            # print(layer, hasattr(layer, 'conv'))\n",
        "            # only add the channels to the feature extractor channels if it is in the list of layers\n",
        "            if layer_id in layers:\n",
        "                if hasattr(layer, 'conv'):\n",
        "                    try:\n",
        "                        conv_out = layer.conv[2][0]\n",
        "                    except:\n",
        "                        print(\"The resblock has a different configuration as expected\")\n",
        "                else:\n",
        "                    try:\n",
        "                        if type(layer[0]) == nn.Conv2d:\n",
        "                            conv_out = layer[0]\n",
        "                    except:\n",
        "                        print(\"The conv layer has a different configuration as expected\")\n",
        "                self.feature_extractor_channels.append(conv_out.out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x, layers=[], encode_only=False):\n",
        "        \"\"\"\n",
        "        Generator forward pass; only forward the specific layers if they were passed (if no layers, return entire encoder + decoder)\n",
        "        \"\"\"\n",
        "        # TODO: Consider removing the layers input var\n",
        "        layers = self.nce_layers\n",
        "        # only output specific generator encoder layers\n",
        "        if len(layers) > 0 and encode_only:\n",
        "            encoder_layer_outs = [] # list of activation maps when one index corresponds to a layer of the encoder\n",
        "            for layer_id, layer in enumerate(self.encoder):\n",
        "                # compute the output for the layer\n",
        "                x = layer(x)\n",
        "                # print('gen shapes', x.shape, layer_id)\n",
        "                # only add the layer activation map to the output if in the list of layers (this will be used as one of the layers by PatchNCELoss)\n",
        "                if layer_id in layers:\n",
        "                    encoder_layer_outs.append(x)\n",
        "                    # print('gen shapes', x.shape[2:])\n",
        "            # raise Exception('done gen fwd test')\n",
        "            return encoder_layer_outs\n",
        "\n",
        "        # first apply encoder - only reaches this part if layers is empty\n",
        "        enc_x = self.encoder(x)\n",
        "        # print(enc_x.shape)\n",
        "        # return only encoder results for patchNCELoss\n",
        "        if encode_only:\n",
        "            return enc_x\n",
        "        \n",
        "        # second apply decoder\n",
        "        dec_x = self.decoder(enc_x)\n",
        "        return dec_x\n",
        "\n",
        "\n",
        "class Disciminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Create a discriminator model to tell the difference between real and fake images (assumes 128*128 input images)\n",
        "    \"\"\"\n",
        "    def __init__(self, ch_in, base_ch=64, n_layers=3, n_downsamples=3):\n",
        "        super().__init__()\n",
        "        self.ch_in = ch_in\n",
        "        self.base_ch = base_ch\n",
        "        self.n_layers = n_layers\n",
        "        self.n_downsamples = n_downsamples\n",
        "        self.convs = self._create_conv_discriminator()\n",
        "    \n",
        "    def _create_conv_discriminator(self):\n",
        "        # start with a 1x1 conv assuming  a 128*128 input image; convert to base_ch channels\n",
        "        convs = [_single_conv(self.ch_in, self.base_ch, 1, stride=2, leaky=True)]\n",
        "\n",
        "        # add multiple res blocks to reduce the 128*128 input\n",
        "        ch_mult_prev = 1\n",
        "        ch_mult = 1\n",
        "        # first layer was alreday applied above; apply all others\n",
        "        for i in range(1, self.n_layers):\n",
        "            ch_mult_prev = ch_mult\n",
        "            # set the multiplier to a max of 8 or 2**current layer\n",
        "            ch_mult = min(2**i, 8)\n",
        "            convs += [\n",
        "                ResBlock(self.base_ch * ch_mult_prev, self.base_ch * ch_mult, stride=2 if i < self.n_downsamples else 1, leaky=True)\n",
        "            ]\n",
        "        \n",
        "        curr_ch = self.base_ch * ch_mult\n",
        "        for j in range(self.n_layers):\n",
        "          convs += [\n",
        "                ResBlock(curr_ch, curr_ch//2, leaky=True)\n",
        "            ]\n",
        "          curr_ch = curr_ch//2\n",
        "        # output a single channel feature map of activations from the discriminator (from the Patch GAN paper)\n",
        "        convs += [_single_conv(curr_ch, 1, 3, leaky=True)]\n",
        "        return nn.Sequential(*convs)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # apply the convolutional discriminator\n",
        "        out = self.convs(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class EncoderFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Create a MLP (multilayer perceptron) to transform the patch features from output and input into shared feature space\n",
        "    Approach is taken from SimCLR: https://arxiv.org/pdf/2002.05709.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, nce_layer_channels, n_features=256):\n",
        "        \"\"\"\n",
        "        Creates the MLP network (H sub l in the paper) that will transform input encoder patches to a shared embedding space\n",
        "\n",
        "        Args: \n",
        "            nce_layer_channels: A list containing the size of the channels for each of the layers that will be used by nce loss\n",
        "            n_features: An integer which is the number of features that the transformed space will have\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.norm = Normalize(2)\n",
        "        # self.gpu = gpu\n",
        "        self.nce_layer_channels = nce_layer_channels\n",
        "        self.n_features = n_features\n",
        "        # create the mlp for each layer that will be used for nce (based on the channels)\n",
        "        self.mlps = nn.ModuleList(self._create_mlp()) # define this as a module list for pytorch to treat each module in the list on it's own\n",
        "    \n",
        "    def _create_mlp(self):\n",
        "        \"\"\"\n",
        "        Create a two layer fully connected network for each of the layers with the corresponding channel sizes\n",
        "        \"\"\"\n",
        "        mlps = []\n",
        "        for ch_in in self.nce_layer_channels:\n",
        "            mlps.append(nn.Sequential(*[\n",
        "                nn.Linear(ch_in, self.n_features), \n",
        "                nn.ReLU(inplace=True), \n",
        "                nn.Linear(self.n_features, self.n_features)\n",
        "                ]))\n",
        "        return mlps\n",
        "\n",
        "\n",
        "    def forward(self, feats, num_patches, patch_ids=None):\n",
        "        \"\"\"\n",
        "        Performs a forward pass for an EncoderFeatureExtractor (called Hl in this paper: https://arxiv.org/pdf/2007.15651.pdf)\n",
        "\n",
        "        Args: \n",
        "            feats: A list containing tensor features passed from specific layer of the generator encoder (assume tensors of size bs*channels*H*W)\n",
        "            num_patches: The number of patches to sample from feats\n",
        "            patch_ids: The indexes of patches to select from each layer of feats (this is != None when a forward call has been used on the source images and we want to take the same patches from the target images)\n",
        "        \n",
        "        Returns: \n",
        "            A tuple of lists, the first list being \n",
        "        \"\"\"\n",
        "        return_ids = []\n",
        "        return_feats = []\n",
        "\n",
        "        # go through each of the feature layers while grabbing corresponding mlp\n",
        "        for feat_id, (mlp, feat) in enumerate(zip(self.mlps, feats)):\n",
        "            # reshape the feature tensor to be (bs*img_locs*channels)\n",
        "            feat_reshaped = feat.flatten(2, 3).permute(0, 2, 1)\n",
        "            # print('feats', feat.shape, feat_reshaped.shape)\n",
        "\n",
        "            if num_patches > 0:\n",
        "                # get the patch id for the current layer if the patch_ids exist\n",
        "                if patch_ids is not None:\n",
        "                    patch_id = patch_ids[feat_id]\n",
        "                # create the patches if the patch ids DNE\n",
        "                else:\n",
        "                    # get random permutation of all indices from 0 to max img loc (axis=1)\n",
        "                    patch_id = rng.permutation(feat_reshaped.shape[1])\n",
        "                    # print(num_patches, len(patch_id))\n",
        "                    # index the patch_id to extract first num_patch locations \n",
        "                    if num_patches < len(patch_id):\n",
        "                        patch_id = patch_id[:num_patches]\n",
        "                # index only the patches that will be used from feats_reshaped (axis 1 is the img_loc axis)\n",
        "                # note that in practice, bs=1 so that we only compare batches in the same image (internal patches outperforms external patches)\n",
        "                # TODO: Consider removing the .view() at the end of the next line\n",
        "                feat_patch_sampled = feat_reshaped[:, patch_id, :].view(-1, feat_reshaped.shape[2]) # flatten the tensor to be of shape (patch_loc, channels). The PatchNCE loss will be done regardless of batch (this makes it by patches and not by image)\n",
        "            else:\n",
        "                # the number of patches is zero or negative; take all patches \n",
        "                feat_patch_sampled = feat_reshaped\n",
        "                patch_id = []\n",
        "             \n",
        "            # apply the mlp (H sub l) to select patches\n",
        "            feat_sampled_emb_space = mlp(feat_patch_sampled)\n",
        "            # \"We normalize vectors onto a unit sphere to prevent the space from collapsing or expanding\"\n",
        "            feat_norm = self.norm(feat_sampled_emb_space)\n",
        "            return_ids.append(patch_id)\n",
        "\n",
        "            # if zero patches, return in as similar a shape as possible (but in dimensions of shared embedding space after passing through mlp)\n",
        "            if num_patches == 0:\n",
        "                batch, _, h, w = feat.shape\n",
        "                n_emb_feat = feat_norm.shape[-1]\n",
        "                # shape is (bs*img_loc*emb_features) -> (bs*emb_features*img_loc)\n",
        "                feat_norm = feat_norm.permute(0, 2, 1).view(batch, n_emb_feat, h, w)\n",
        "            return_feats.append(feat_norm)\n",
        "        \n",
        "        return return_feats, return_ids\n",
        "\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7HMnmw33ooiZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class DGANLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines the GAN loss function for the discriminator's predictions of real or fake on data\n",
        "    \"\"\"\n",
        "    def __init__(self, mode, device):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.device=device\n",
        "\n",
        "        # define the loss to be used when receiving a grid of activaitons (predictions) for an image\n",
        "        if self.mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif self.mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif self.mode == 'non-saturating':\n",
        "            self.loss = None\n",
        "        else:\n",
        "            raise NotImplementedError(f\"The mode {mode} for DGANLoss is not implemented\")\n",
        "    \n",
        "    def create_targ_tensor(self, inp, is_real):\n",
        "        if is_real:\n",
        "            # apply some label smoothing for better generalization\n",
        "            targ_tensor = torch.Tensor([0.9]).to(self.device)\n",
        "        else:\n",
        "            targ_tensor = torch.Tensor([0.1]).to(self.device)\n",
        "        # returns the target tensor in the same shape as the input (since it will be a grid of activations from the discriminator)\n",
        "        return targ_tensor.expand_as(inp)\n",
        "        \n",
        "    \n",
        "    def forward(self, x, is_real):\n",
        "        if self.mode in ['lsgan', 'vanilla']:\n",
        "            # create an equal shaped target tensor and compute the loss\n",
        "            targ_tens = self.create_targ_tensor(x, is_real)\n",
        "            loss = self.loss(x, targ_tens)\n",
        "        # non-saturating loss is being used\n",
        "        else:\n",
        "            if is_real:\n",
        "                # minimize the loss by passing softplus(-x) = ln(1+e**-x) as x -> +inf (real prediction get predicted more real) => e**-x -> 0 => softplus(-x) -> 0+\n",
        "                loss = F.softplus(-x)\n",
        "            else:\n",
        "                # minimize the loss by passing softplus(x) = ln(1+e**x) as x -> -inf (fake prediction get predicted more fake) => e**x -> 0 => softplus(x) -> 0+\n",
        "                loss = F.softplus(x)\n",
        "\n",
        "            # since the discriminator is giving a grid of activations, group the loss by batch and take the mean along the activation dimension\n",
        "            loss = loss.view(x.shape[0], -1).mean(1)\n",
        "        return loss\n",
        "\n",
        "# TODO: Ask prof for help understanding math behind this loss function\n",
        "class PatchNCELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The patch NCE loss to associate similar sections in source and target images\n",
        "    \"\"\"\n",
        "    def __init__(self, tau, bs):\n",
        "        super().__init__()\n",
        "        self.loss = nn.CrossEntropyLoss(reduction='none')\n",
        "        # assume bs=1 by default\n",
        "        self.bs = bs\n",
        "        # division factor for scaling outputs\n",
        "        self.tau = tau\n",
        "    \n",
        "    def forward(self, real_feats, fake_feats):\n",
        "        n_patches = real_feats.shape[0]\n",
        "        n_transformed_space = real_feats.shape[1]\n",
        "        # detach generator for the real features\n",
        "        fake_feats = fake_feats.detach()\n",
        "\n",
        "        # create the positive feature results by doing (1, bs*n_transformed_space) * (bs*n_transformed_space, 1) = (1, 1) for each patch in the group\n",
        "        l_pos = torch.bmm(real_feats.view(n_patches, 1, -1), fake_feats.view(n_patches, -1, 1)).view(n_patches, 1)\n",
        "\n",
        "        real_feats = real_feats.view(self.bs, -1, n_transformed_space)\n",
        "        fake_feats = fake_feats.view(self.bs, -1, n_transformed_space)\n",
        "        # the new number of patches is the number of patches by image\n",
        "        n_patches = real_feats.shape[1]\n",
        "        # create the negative feature results by doing (n_patches, n_transformed_space) * (n_transformed_space, n_patches) = (n_patches, n_patches)\n",
        "        l_neg_batch = torch.bmm(real_feats, fake_feats.transpose(2, 1))\n",
        "        # remove meaningless diagonal entries by masking the l_neg_batch with an identity matrix\n",
        "        diag = torch.eye(n_patches, device=real_feats.device).bool()\n",
        "        l_neg_batch.masked_fill_(diag, -10.0)\n",
        "        l_neg = l_neg_batch.view(n_patches, -1) # NOTICE: this line is different from the paper\n",
        "\n",
        "        # concat over patch dimension (1 pos and n_patch neg patches)\n",
        "        out = torch.cat([l_pos, l_neg], dim=1) / self.tau\n",
        "        # the target feature is alway the l_pos feature of out which is at index 0\n",
        "        targs = torch.zeros(out.shape[0], device=real_feats.device).long()\n",
        "        loss = self.loss(out, targs)\n",
        "        return loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bVNTuN0WosdV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# from create_gen_discr import Generator, Disciminator, EncoderFeatureExtractor\n",
        "# from losses import DGANLoss, PatchNCELoss\n",
        "# from data_utility import set_requires_grad\n",
        "import os\n",
        "\n",
        "class CUT_gan(nn.Module):\n",
        "    def __init__(self, lambda_gan, lambda_nce, nce_layers, device, lr, nce_idt=True, encoder_net_features=256, nce_tau=0.07, num_patches=256, train=True, gan_l_type='non-saturating', bs=1):\n",
        "        \"\"\"\n",
        "        Creates a CUT model which is a type of GAN for image to image translation\n",
        "\n",
        "        Args: \n",
        "            lambda_gan: The weight for the GAN loss for the generator (since generator loss depends on gan and nce loss)\n",
        "            lambda_nce: The weight for the NCE loss for the generator (since generator loss depends on gan and nce loss)\n",
        "            nce_layers: A list of layers that the generator encoder will return for the nce_loss from convolutional layer (can also be residual blocks) activations\n",
        "            device: torch.Device('cpu') if a cpu is used to train on otherwise torch.Device('cuda:0') if gpu is avaiable\n",
        "            lr: The learning rate for stepping the weights of all of the optimizers\n",
        "            nce_idt: True if the loss consists of the identity loss NCE(Y, X-tilde), False otherwise\n",
        "            encoder_net_features: The number of features that the EncoderFeatureExtractor will produce in it's new space\n",
        "            nce_tau: A constant that the nce loss will use to scale the matrices by in the loss\n",
        "            num_patches: The number of pathces that will be used by nce to compute the loss\n",
        "            train: True if training False if evaluating/inferencing\n",
        "            gan_l_type: The type of dgan loss to be used (either 'non-saturating', 'vanilla', or 'lsgan')\n",
        "            bs: The batch size that is going to be used in training\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # keep relevant attirbutes for the training loop\n",
        "        self.device = device\n",
        "        self.lambda_gan = lambda_gan\n",
        "        self.lambda_nce = lambda_nce\n",
        "        self.nce_idt = nce_idt\n",
        "        self.num_patches = num_patches\n",
        "        self.nce_layers = nce_layers\n",
        "        # definte the generator for the CUT model to go from rgb -> rgb image\n",
        "        self.gen = Generator(3, 3, nce_layers).to(self.device)\n",
        " \n",
        "        if train:\n",
        "            # define a discriminator to take 3 input channels with 4 residual blocks\n",
        "            self.disc = Disciminator(3, n_layers=4).to(self.device)\n",
        "            # define a feature extractor network H sub l to transform generator encoder features to a new embedding space for nce loss\n",
        "            self.feat_net = EncoderFeatureExtractor(self.gen.feature_extractor_channels, n_features=encoder_net_features).to(self.device)\n",
        "\n",
        "            # define loss functions\n",
        "            self.dgan_loss = DGANLoss(gan_l_type, self.device).to(self.device)\n",
        "            self.nce_losses = []\n",
        "            for _ in nce_layers:\n",
        "                self.nce_losses.append(PatchNCELoss(nce_tau, bs).to(self.device))\n",
        "\n",
        "            # create adam optimizers\n",
        "            self.gen_optim = optim.Adam(self.gen.parameters(), lr=lr)\n",
        "            self.disc_optim = optim.Adam(self.disc.parameters(), lr=lr)\n",
        "            self.feat_net_optim = optim.Adam(self.feat_net.parameters(), lr=lr)\n",
        "    \n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Set all 3 networks to training mode\n",
        "        \"\"\"\n",
        "        self.gen.train()\n",
        "        self.disc.train()\n",
        "        self.feat_net.train()\n",
        "    \n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Depending on the mode, set the networks to eval mode\n",
        "        \"\"\"\n",
        "        if self.train:\n",
        "            self.gen.eval()\n",
        "            self.disc.eval()\n",
        "            self.feat_net.eval()\n",
        "        else:\n",
        "            self.gen.eval()\n",
        "    \n",
        "    def save_nets(self, epoch, folder='models'):\n",
        "        \"\"\"\n",
        "        Save the network params on the cpu for all 3 networks\n",
        "        \"\"\"\n",
        "        gen_checkpoint = {'state_dict': self.gen.cpu().state_dict()}\n",
        "        disc_checkpoint = {'state_dict': self.disc.cpu().state_dict()}\n",
        "        feat_checkpoint = {'state_dict': self.feat_net.cpu().state_dict()}\n",
        "        \n",
        "        # move the nets back to the current device\n",
        "        self.gen.to(self.device)\n",
        "        self.disc.to(self.device)\n",
        "        self.feat_net.to(self.device)\n",
        "\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        get_path = lambda model_name: os.path.join(folder, f\"{epoch}_{model_name}.pth\")\n",
        "        torch.save(gen_checkpoint, get_path('gen'))\n",
        "        torch.save(disc_checkpoint, get_path('disc'))\n",
        "        torch.save(feat_checkpoint, get_path('feat_net'))\n",
        "    \n",
        "    def load_gen(self, epoch, folder='models'):\n",
        "        checkpoint = torch.load(os.path.join(folder, f\"{epoch}_gen.pth\"), map_location=torch.device('cpu'))\n",
        "        gen = Generator(3, 3, self.nce_layers)\n",
        "        gen.load_state_dict(checkpoint['state_dict'])\n",
        "        # print(next(self.gen.parameters()))\n",
        "        return gen\n",
        "\n",
        "\n",
        "    def forward(self, real_src, real_targ=None):\n",
        "        \"\"\"\n",
        "        Does a forward pass of the generator for training and inference.\n",
        "\n",
        "        Saves the real source, real target, and fake target images.\n",
        "        Also, if nce_idt and train are True, saves the fake source images. \n",
        "        \"\"\"\n",
        "        # save the current real src and targ images to pass to other functions\n",
        "        self.real_src = real_src\n",
        "        self.real_targ = real_targ\n",
        "        if self.train and self.nce_idt and real_targ != None:\n",
        "            # put the real source and target images if in training and using identity nce loss\n",
        "            real = torch.cat((real_src, real_targ), dim=0)\n",
        "        else:\n",
        "            real = real_src\n",
        "        \n",
        "        # use the generator on real images\n",
        "        fake = self.gen(real)\n",
        "        # get fake target images (y hat)\n",
        "        self.fake_targ = fake[:real_src.shape[0]]\n",
        "        # if possible, get fake source images for identity loss (x tilde)\n",
        "        if self.train and self.nce_idt and real_targ != None:\n",
        "            self.fake_src = fake[real_src.shape[0]:]\n",
        "\n",
        "\n",
        "    def optimize_params(self, real_src, real_targ, discriminator_train=1):\n",
        "        \"\"\"\n",
        "        Forward pass, loss, back propagate, and step for all 3 networks to optmize all the params\n",
        "        \"\"\"\n",
        "        # forward pass\n",
        "        self.forward(real_src, real_targ)\n",
        "\n",
        "        # discriminator param update\n",
        "        for _ in range(discriminator_train):\n",
        "            set_requires_grad(self.disc, True)\n",
        "            self.disc_optim.zero_grad()\n",
        "            self.loss_d = self.calc_d_loss()\n",
        "            self.loss_d.backward()\n",
        "            self.disc_optim.step()\n",
        "\n",
        "        # generator and encoder feature extractor param update\n",
        "        set_requires_grad(self.disc, False)\n",
        "        self.gen_optim.zero_grad()\n",
        "        self.feat_net_optim.zero_grad()\n",
        "        self.loss_g = self.calc_g_loss()\n",
        "        self.loss_g.backward()\n",
        "        self.gen_optim.step()\n",
        "        self.feat_net_optim.step()\n",
        "\n",
        "\n",
        "    def calc_d_loss(self):\n",
        "        \"\"\"\n",
        "        Calculates discriminator loss\n",
        "        \"\"\"\n",
        "        # prevent generator from updating\n",
        "        fake_targ = self.fake_targ.detach()\n",
        "        # fake target loss\n",
        "        fake_pred = self.disc(fake_targ)\n",
        "        self.fake_d_loss = self.dgan_loss(fake_pred, False)\n",
        "        # real target loss\n",
        "        real_pred = self.disc(self.real_targ)\n",
        "        self.real_d_loss = self.dgan_loss(real_pred, True)\n",
        "        # combine both fake and real target loss\n",
        "        return (self.fake_d_loss + self.real_d_loss) * 0.5\n",
        "\n",
        "\n",
        "    def calc_g_loss(self):\n",
        "        \"\"\"\n",
        "        Calculates generator loss\n",
        "        \"\"\"\n",
        "        # check normal GAN loss on discriminator with fake generator images\n",
        "        pred_fake = self.disc(self.fake_targ)\n",
        "        self.gan_g_loss = self.dgan_loss(pred_fake, False) * self.lambda_gan\n",
        "\n",
        "        # use patch NCE loss for src -> fake targ\n",
        "        self.nce_loss = self.calc_nce_loss(self.real_src, self.fake_targ)\n",
        "        # use patch NCE loss for targ -> fake source (identity loss)\n",
        "        self.nce_identity_loss = self.calc_nce_loss(self.real_targ, self.fake_src)\n",
        "        # get total nce loss\n",
        "        nce_loss_total = (self.nce_loss + self.nce_identity_loss) * 0.5\n",
        "        # get total loss (Lgan + NCE loss + identity NCE loss)\n",
        "        loss_total = nce_loss_total + self.gan_g_loss\n",
        "        return loss_total\n",
        "\n",
        "\n",
        "    def calc_nce_loss(self, src, targ):\n",
        "        \"\"\"\n",
        "        Calculates the NCE loss using patches to associate similar locations and dissociate different locations\n",
        "        \"\"\"\n",
        "        # get the pathces for source after doing H sub l(G enc(x))\n",
        "        src_feats = self.gen(src, encode_only=True)\n",
        "        transformed_src_feats, patch_ids = self.feat_net(src_feats, self.num_patches)\n",
        "\n",
        "        # get the patches for target after doing H sub l(G enc(G(x))) \n",
        "        targ_feats = self.gen(targ, encode_only=True)\n",
        "        transformed_targ_feats, _ = self.feat_net(targ_feats, self.num_patches, patch_ids=patch_ids)\n",
        "\n",
        "        total_loss = 0\n",
        "        # calculate the loss for each layer in the transformed returned features\n",
        "        for src_feat, targ_feat, nce_loss in zip(transformed_src_feats, transformed_targ_feats, self.nce_losses):\n",
        "            # TODO: Consider switching src_feats and targ_feats if training is not working well\n",
        "            total_loss += (nce_loss(targ_feat, src_feat) * self.lambda_nce).mean()\n",
        "        \n",
        "        return total_loss/len(self.gen.nce_layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AvwQDtjozk2",
        "outputId": "f9ccf447-46a6-48c8-d6d3-fca86b43f009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 258/995 [01:07<03:08,  3.91it/s]"
          ]
        }
      ],
      "source": [
        "# from data_utility import Data, save_images, report_losses\n",
        "# from cut_model import CUT_gan\n",
        "import torch\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# define the params to pass to the cut gan model\n",
        "lambda_gan = 1\n",
        "lambda_nce = 1\n",
        "nce_layers = [0, 1, 2, 3, 4, 6, 8, 10]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "enc_net_feats = 16\n",
        "num_patches = 512\n",
        "print(f'Device: {device}')\n",
        "lr = 2e-3 # use the lr as recommended by the paper\n",
        "gan_l_type='lsgan' # consider switching to lsgan as used in the paper\n",
        "bs = 1\n",
        "\n",
        "save_every = 100 # save generator image every x images in a batch\n",
        "# define the number of epochs for training\n",
        "epochs = 30\n",
        "loss_names = ['DLoss', 'FakeDLoss', 'RealDLoss', 'GLoss', 'GANGLoss', 'NCELoss', 'NCEIdentityLoss']\n",
        "\n",
        "def main():\n",
        "    data = Data('apples', 'oranges', False, bs, (128, 128))\n",
        "    data.get_loaders('apples_and_oranges')\n",
        "    # show_batch(data, 9)\n",
        "\n",
        "    # define the CUT gan model which has all 3 nets and training loop for the 3 nets\n",
        "    cut_model = CUT_gan(lambda_gan, lambda_nce, nce_layers, device, lr, gan_l_type=gan_l_type, bs=bs, num_patches=num_patches, encoder_net_features=enc_net_feats)\n",
        "    # print(cut_model.disc)\n",
        "    # print(next(cut_model.gen.parameters()).device)\n",
        "    # print(cut_model.gen.feature_extractor_channels)\n",
        "    # raise Exception('done')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # discriminator losses\n",
        "        loss_d = 0\n",
        "        fake_d_loss = 0\n",
        "        real_d_loss = 0\n",
        "        # generator losses\n",
        "        loss_g = 0\n",
        "        gan_g_loss = 0\n",
        "        nce_loss = 0\n",
        "        nce_identity_loss = 0\n",
        "        start_ep = time.time()\n",
        "        x_check = []\n",
        "        with tqdm(total=len(data.dlSourceTrain)) as pbar:\n",
        "            for i, ((x, _), (y, _)) in enumerate(zip(data.dlSourceTrain, data.dlTargetTrain)):\n",
        "                # set model to train\n",
        "                cut_model.train()\n",
        "                # move the image tensors onto the correct device\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                # train model\n",
        "                cut_model.optimize_params(x, y)\n",
        "                # update losses\n",
        "                # with torch.no_grad():\n",
        "                loss_d += cut_model.loss_d.item()\n",
        "                fake_d_loss += cut_model.fake_d_loss.item()\n",
        "                real_d_loss += cut_model.real_d_loss.item()\n",
        "                loss_g += cut_model.loss_g.item()\n",
        "                gan_g_loss += cut_model.gan_g_loss.item()\n",
        "                nce_loss += cut_model.nce_loss.item()\n",
        "                nce_identity_loss += cut_model.nce_identity_loss.item()\n",
        "                # save image to show in results at end of epoch\n",
        "                if i % save_every == 0:\n",
        "                    x_check.append(x)\n",
        "                # update progress\n",
        "                pbar.update(bs)\n",
        "                # if i == 20:\n",
        "                #     break\n",
        "    \n",
        "\n",
        "        # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "        ep_time = time.time() - start_ep\n",
        "        cut_model.save_nets(epoch) # save all 3 networks\n",
        "        # output training loss and time\n",
        "        loss_list = [loss_d, fake_d_loss, real_d_loss, loss_g, gan_g_loss, nce_loss, nce_identity_loss]\n",
        "        report_losses(loss_list, loss_names, epoch, ep_time, i+1)\n",
        "        # output visuals\n",
        "        cut_model.eval()\n",
        "        x_check = torch.cat(x_check, dim=0)\n",
        "        # print(x_check.device)\n",
        "        # print(next(cut_model.gen.parameters()).device)\n",
        "        # raise Exception('done')\n",
        "        cut_model(x_check)\n",
        "        x_fake = cut_model.fake_targ\n",
        "        # put the images together to save them\n",
        "        ims = torch.cat((x_check, x_fake), dim=3)\n",
        "        save_images(ims, f'ep{epoch}.png')\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qOBbTs6pBoe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cut_gan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
